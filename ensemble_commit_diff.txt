commit a1f4a1088ac7f4aec92ccdd897590333fca9be3a
Author:     Sujaanb <sujaanbhattacharyya@gmail.com>
AuthorDate: Thu Feb 5 19:17:33 2026 +0530
Commit:     Sujaanb <sujaanbhattacharyya@gmail.com>
CommitDate: Thu Feb 5 19:17:33 2026 +0530

    feat: Introduce AASIST ensemble detector for AI-generated voice detection with multi-window inference and language heuristics, and update API key comparison to be case-insensitive.

diff --git a/.env.example b/.env.example
index 03e1572..8220d3c 100644
--- a/.env.example
+++ b/.env.example
@@ -5,24 +5,33 @@
 # REQUIRED SETTINGS
 # =============================================================================
 
-# API Keys (comma-separated list of valid keys)
+# API Keys (comma-separated list of valid keys, case-insensitive)
 # Example: VOICE_API_KEYS=key1,key2,key3
 VOICE_API_KEYS=your-secret-api-key-here
 
 # =============================================================================
-# AASIST MODEL CONFIGURATION
+# AASIST ENSEMBLE MODEL CONFIGURATION
 # =============================================================================
 
-# Path to TorchScript model file (.pt)
-# Export your fine-tuned AASIST model using: torch.jit.save(traced_model, "model.pt")
-AASIST_MODEL_PATH=models/aasist_finetuned.pt
-
 # Device for inference: "cpu" or "cuda"
 AASIST_DEVICE=cpu
 
+# GCS URIs for model weights (gs://bucket/path format)
+# Leave empty to use local paths only (for local development)
+AASIST_ORIG_WEIGHTS_GCS_URI=
+AASIST_FT_WEIGHTS_GCS_URI=
+
+# Local cache paths for downloaded weights
+# Cloud Run uses /tmp for ephemeral storage
+AASIST_ORIG_CACHE_PATH=/tmp/aasist_original.pth
+AASIST_FT_CACHE_PATH=/tmp/aasist_finetuned_best.pth
+
 # Classification threshold (spoof probability > threshold = AI_GENERATED)
 AASIST_THRESHOLD=0.5
 
+# Multi-window inference: number of windows to sample from long audio
+AASIST_MAX_WINDOWS=3
+
 # =============================================================================
 # OPTIONAL SETTINGS (defaults shown)
 # =============================================================================
diff --git a/README.md b/README.md
index aaf3f9b..065f530 100644
--- a/README.md
+++ b/README.md
@@ -2,7 +2,7 @@
 
 FastAPI backend for detecting AI-generated speech vs human speech. Supports **Tamil**, **English**, **Hindi**, **Malayalam**, and **Telugu** languages.
 
-Built for the hackathon "AI-Generated Voice Detection" with production deployment on Google Cloud Run.
+Uses **AASIST ensemble inference** (original + fine-tuned models) with heuristic pattern rules for improved multilingual detection. Built for production deployment on **Google Cloud Run (CPU-only)**.
 
 ## Quick Start
 
@@ -18,24 +18,34 @@ pip install -r requirements.txt
 # Copy example config
 cp .env.example .env
 
-# Edit .env and set your API key and model path
+# Edit .env and set your API key and model paths
 # VOICE_API_KEYS=your-secret-key-here
-# AASIST_MODEL_PATH=models/aasist_finetuned.pt
+# AASIST_ORIG_WEIGHTS_GCS_URI=gs://bucket/original.pth
+# AASIST_FT_WEIGHTS_GCS_URI=gs://bucket/finetuned.pth
 ```
 
 ### 3. Run Locally
 
 ```bash
-# Option 1: Direct Python
+# Start the server
 python api.py
 
-# Option 2: Use the script (Linux/Mac)
-chmod +x scripts/run_local.sh
-./scripts/run_local.sh
+# Or use uvicorn directly
+uvicorn api:app --host 0.0.0.0 --port 8001 --reload
 ```
 
 Server runs at `http://localhost:8001` with docs at `/docs` (when `ENABLE_DOCS=1`).
 
+### 4. Test the API
+
+```bash
+# Python test script
+python scripts/test_local.py --audio your_audio.mp3 --api-key your-key
+
+# Or curl
+./scripts/test_curl.sh your_audio.mp3 your-key
+```
+
 ---
 
 ## API Reference
@@ -48,10 +58,10 @@ POST /api/voice-detection
 
 ### Headers
 
-| Header         | Required | Description                |
-| -------------- | -------- | -------------------------- |
-| `Content-Type` | Yes      | Must be `application/json` |
-| `x-api-key`    | Yes      | Your API key               |
+| Header         | Required | Description                         |
+| -------------- | -------- | ----------------------------------- |
+| `Content-Type` | Yes      | Must be `application/json`          |
+| `x-api-key`    | Yes      | Your API key (case-insensitive)     |
 
 ### Request Body
 
@@ -77,7 +87,7 @@ POST /api/voice-detection
   "language": "English",
   "classification": "AI_GENERATED",
   "confidenceScore": 0.87,
-  "explanation": "AASIST anti-spoof model detected patterns consistent with synthetic speech."
+  "explanation": "Ensemble: English FAKE (direct detection). Orig(fake=93%), FT(real=45%)"
 }
 ```
 
@@ -90,142 +100,92 @@ POST /api/voice-detection
 }
 ```
 
-### Example curl
-
-```bash
-# Encode your MP3 file
-AUDIO_BASE64=$(base64 -w 0 your_audio.mp3)
-
-# Make request
-curl -X POST http://localhost:8001/api/voice-detection \
-  -H "Content-Type: application/json" \
-  -H "x-api-key: your-api-key" \
-  -d "{
-    \"language\": \"English\",
-    \"audioFormat\": \"mp3\",
-    \"audioBase64\": \"${AUDIO_BASE64}\"
-  }"
-```
-
 ---
 
 ## Configuration
 
-All settings are configurable via environment variables:
-
-| Variable                  | Default                      | Description                             |
-| ------------------------- | ---------------------------- | --------------------------------------- |
-| `VOICE_API_KEYS`          | (required)                   | Comma-separated valid API keys          |
-| `AASIST_MODEL_PATH`       | `models/aasist_finetuned.pt` | Path to TorchScript model file          |
-| `AASIST_DEVICE`           | `cpu`                        | Device for inference: `cpu` or `cuda`   |
-| `AASIST_THRESHOLD`        | `0.5`                        | Classification threshold                |
-| `ENABLE_DOCS`             | `0`                          | Set to `1` to enable `/docs`            |
-| `MAX_MP3_BYTES`           | `15000000`                   | Max decoded MP3 size (15 MB)            |
-| `MAX_DURATION_SECONDS`    | `300`                        | Max audio duration (5 minutes)          |
-| `MIN_DURATION_SECONDS`    | `0.5`                        | Min audio duration                      |
-| `SILENCE_RATIO_THRESHOLD` | `0.80`                       | Silence ratio for low-confidence result |
-| `PORT`                    | `8080`                       | Server port (Cloud Run sets this)       |
+| Variable                     | Default                          | Description                             |
+| ---------------------------- | -------------------------------- | --------------------------------------- |
+| `VOICE_API_KEYS`             | (required)                       | Comma-separated valid API keys          |
+| `AASIST_DEVICE`              | `cpu`                            | Device: `cpu` or `cuda`                 |
+| `AASIST_ORIG_WEIGHTS_GCS_URI`| (empty)                          | GCS URI for original model weights      |
+| `AASIST_FT_WEIGHTS_GCS_URI`  | (empty)                          | GCS URI for fine-tuned model weights    |
+| `AASIST_ORIG_CACHE_PATH`     | `/tmp/aasist_original.pth`       | Local cache path for original weights   |
+| `AASIST_FT_CACHE_PATH`       | `/tmp/aasist_finetuned_best.pth` | Local cache path for finetuned weights  |
+| `AASIST_THRESHOLD`           | `0.5`                            | Classification threshold                |
+| `AASIST_MAX_WINDOWS`         | `3`                              | Max windows for long audio              |
+| `ENABLE_DOCS`                | `0`                              | Set to `1` to enable `/docs`            |
+| `MAX_MP3_BYTES`              | `15000000`                       | Max decoded MP3 size (15 MB)            |
+| `MAX_DURATION_SECONDS`       | `300`                            | Max audio duration (5 minutes)          |
 
 ---
 
-## AASIST Detector
+## Ensemble Model Architecture
 
-This API uses the **AASIST** (Audio Anti-Spoofing using Integrated Spectro-Temporal) model for detecting AI-generated speech.
+This API uses an **ensemble of two AASIST models**:
 
-### Model Requirements
+1. **Original AASIST** - English-trained baseline
+2. **Fine-tuned AASIST** - Multilingual fine-tuned model
 
-1. **Export your fine-tuned AASIST model as TorchScript**:
-   ```python
-   # In your training pipeline
-   traced_model = torch.jit.trace(model, example_input)
-   torch.jit.save(traced_model, "aasist_finetuned.pt")
-   ```
+### Multi-Window Inference
 
-2. **Place the `.pt` file** at the path specified by `AASIST_MODEL_PATH`
+For audio longer than ~4 seconds, the detector samples multiple evenly-spaced windows (default: 3) and averages scores.
 
-3. **Model output format**: The model should output logits of shape `[batch, 2]` where:
-   - Index 0 = spoof score (AI-generated)
-   - Index 1 = bonafide score (human)
+### Heuristic Pattern Rules
 
-### Classification Logic
+The ensemble uses pattern detection to handle inverted predictions for Indian languages:
 
-- If `softmax(logits)[0] > AASIST_THRESHOLD`: ΓåÆ `AI_GENERATED`
-- Otherwise: ΓåÆ `HUMAN`
-- Confidence score is the probability of the predicted class
+| Pattern | Original Model | Fine-tuned | Result |
+|---------|---------------|------------|--------|
+| Indian REAL | >95% fake | >95% real | HUMAN (inverted) |
+| Indian FAKE | <15% fake | >80% real | AI_GENERATED (inverted) |
+| English FAKE | >92% fake | - | AI_GENERATED |
+| English REAL | 85-92% fake | >95% real | HUMAN |
 
 ---
 
-## Docker & Cloud Run Deployment
-
-### Build Docker Image
-
-```bash
-docker build -t voice-detection-api .
-```
+## Cloud Run Deployment
 
-### Run Locally with Docker
+### Prerequisites
 
-```bash
-docker run -p 8080:8080 \
-  -e VOICE_API_KEYS=your-key \
-  -e AASIST_MODEL_PATH=/app/models/aasist_finetuned.pt \
-  -v /path/to/models:/app/models \
-  voice-detection-api
-```
-
-### Deploy to Cloud Run
+1. Upload model weights to GCS:
+   ```bash
+   gsutil cp aasist_original.pth gs://your-bucket/aasist_original.pth
+   gsutil cp aasist_finetuned_best.pth gs://your-bucket/aasist_finetuned_best.pth
+   ```
 
-```bash
-# Set your project
-export GCP_PROJECT_ID=your-project
-export VOICE_API_KEYS=your-production-key
+2. Set environment variables:
+   ```bash
+   export GCP_PROJECT_ID=your-project
+   export VOICE_API_KEYS=your-production-key
+   export AASIST_ORIG_WEIGHTS_GCS_URI=gs://your-bucket/aasist_original.pth
+   export AASIST_FT_WEIGHTS_GCS_URI=gs://your-bucket/aasist_finetuned_best.pth
+   ```
 
-# Run deployment script
-chmod +x scripts/deploy_cloudrun.sh
-./scripts/deploy_cloudrun.sh
-```
+3. Deploy:
+   ```bash
+   chmod +x scripts/deploy_cloudrun.sh
+   ./scripts/deploy_cloudrun.sh
+   ```
 
-Or manually:
+### Resource Allocation
 
-```bash
-# Build with Cloud Build
-gcloud builds submit --tag gcr.io/$GCP_PROJECT_ID/voice-detection-api
-
-# Deploy
-gcloud run deploy voice-detection-api \
-  --image gcr.io/$GCP_PROJECT_ID/voice-detection-api \
-  --region asia-south1 \
-  --allow-unauthenticated \
-  --set-env-vars="VOICE_API_KEYS=$VOICE_API_KEYS,AASIST_MODEL_PATH=/app/models/aasist_finetuned.pt,ENABLE_DOCS=0"
-```
-
-**Note**: Cloud Run has ~32 MiB request size limit. Keep MP3 files under `MAX_MP3_BYTES` (15 MB default).
+- **Memory**: 4 GiB (two PyTorch models)
+- **CPU**: 2 vCPUs
+- **Timeout**: 300s
+- **Concurrency**: 1 (ML model inference)
 
 ---
 
 ## Testing
 
 ```bash
-# Install test dependencies (already in requirements.txt)
-pip install pytest httpx
-
 # Run all tests
 pytest tests/ -v
 
-# Run specific test file
-pytest tests/test_auth.py -v
-
-# Quick test
-pytest -q
+# Tests mock the detector - no model files needed
 ```
 
-Tests cover:
-- **Authentication**: API key validation
-- **Validation**: Request body validation
-- **Contract**: Response JSON shape verification
-
-**Note**: Tests mock the AASIST detector to avoid requiring a real model file.
-
 ---
 
 ## Project Structure
@@ -233,23 +193,22 @@ Tests cover:
 ```
 Γö£ΓöÇΓöÇ api.py                 # FastAPI app entrypoint
 Γö£ΓöÇΓöÇ config.py              # Settings (pydantic-settings)
-Γö£ΓöÇΓöÇ base_requests.py       # Request/response Pydantic models
 Γö£ΓöÇΓöÇ platform_services.py   # /api/voice-detection endpoint
+Γö£ΓöÇΓöÇ models/
+Γöé   ΓööΓöÇΓöÇ AASIST.py          # AASIST model architecture
 Γö£ΓöÇΓöÇ services/
 Γöé   Γö£ΓöÇΓöÇ audio_io.py        # FFmpeg MP3 decoder
+Γöé   Γö£ΓöÇΓöÇ gcs_weights.py     # GCS download utility
 Γöé   ΓööΓöÇΓöÇ qc.py              # Quality control metrics
 Γö£ΓöÇΓöÇ detectors/
 Γöé   Γö£ΓöÇΓöÇ base.py            # BaseDetector interface
 Γöé   Γö£ΓöÇΓöÇ registry.py        # Detector initialization
-Γöé   ΓööΓöÇΓöÇ aasist_detector.py # AASIST model inference
-Γö£ΓöÇΓöÇ tests/
-Γöé   Γö£ΓöÇΓöÇ conftest.py        # Pytest fixtures
-Γöé   Γö£ΓöÇΓöÇ test_auth.py
-Γöé   Γö£ΓöÇΓöÇ test_validation.py
-Γöé   ΓööΓöÇΓöÇ test_contract.py
+Γöé   ΓööΓöÇΓöÇ aasist_detector.py # Ensemble detector
 Γö£ΓöÇΓöÇ scripts/
-Γöé   Γö£ΓöÇΓöÇ run_local.sh       # Local development
-Γöé   ΓööΓöÇΓöÇ deploy_cloudrun.sh # Cloud Run deployment
+Γöé   Γö£ΓöÇΓöÇ deploy_cloudrun.sh # Cloud Run deployment
+Γöé   Γö£ΓöÇΓöÇ test_local.py      # Python test script
+Γöé   ΓööΓöÇΓöÇ test_curl.sh       # Curl test script
+Γö£ΓöÇΓöÇ tests/
 Γö£ΓöÇΓöÇ Dockerfile
 Γö£ΓöÇΓöÇ requirements.txt
 ΓööΓöÇΓöÇ .env.example
diff --git a/config.py b/config.py
index 09c0a1c..3f61e9e 100644
--- a/config.py
+++ b/config.py
@@ -3,9 +3,6 @@ Configuration settings for AI-Generated Voice Detection API.
 Uses pydantic-settings to load from environment variables and .env file.
 """
 
-import os
-from typing import Optional
-
 from pydantic_settings import BaseSettings, SettingsConfigDict
 
 
@@ -28,15 +25,32 @@ class Settings(BaseSettings):
     # API authentication (comma-separated list of valid API keys)
     VOICE_API_KEYS: str = ""
 
-    # AASIST Model Configuration
-    # Path to TorchScript model file (.pt)
-    AASIST_MODEL_PATH: str = "models/aasist_finetuned.pt"
+    # ==========================================================================
+    # AASIST Ensemble Model Configuration
+    # ==========================================================================
+
     # Device for inference: "cpu" or "cuda"
     AASIST_DEVICE: str = "cpu"
+
+    # GCS URIs for model weights (gs://bucket/path format)
+    # Leave empty to use local paths only
+    AASIST_ORIG_WEIGHTS_GCS_URI: str = ""
+    AASIST_FT_WEIGHTS_GCS_URI: str = ""
+
+    # Local cache paths for downloaded weights
+    AASIST_ORIG_CACHE_PATH: str = "/tmp/aasist_original.pth"
+    AASIST_FT_CACHE_PATH: str = "/tmp/aasist_finetuned_best.pth"
+
     # Classification threshold (spoof probability > threshold = AI_GENERATED)
     AASIST_THRESHOLD: float = 0.5
 
-    # Audio size limits
+    # Multi-window inference: max windows to sample from long audio
+    AASIST_MAX_WINDOWS: int = 3
+
+    # ==========================================================================
+    # Audio Limits
+    # ==========================================================================
+
     MAX_MP3_BYTES: int = 15_000_000  # 15 MB
     MAX_DURATION_SECONDS: float = 300.0  # 5 minutes
     MIN_DURATION_SECONDS: float = 0.5  # 0.5 seconds
@@ -44,7 +58,10 @@ class Settings(BaseSettings):
     # Quality control thresholds
     SILENCE_RATIO_THRESHOLD: float = 0.80  # 80% silence triggers low-confidence
 
-    # Server configuration
+    # ==========================================================================
+    # Server Configuration
+    # ==========================================================================
+
     PORT: int = 8080
 
     def get_api_keys(self) -> list[str]:
diff --git a/detectors/aasist_detector.py b/detectors/aasist_detector.py
index b3c8936..ac0e8d9 100644
--- a/detectors/aasist_detector.py
+++ b/detectors/aasist_detector.py
@@ -1,62 +1,64 @@
 """
-AASIST Detector - Anti-spoofing model for AI-generated voice detection.
+AASIST Ensemble Detector - Two-model ensemble for AI-generated voice detection.
 
-Uses a fine-tuned AASIST model exported as TorchScript for inference.
-The model distinguishes between bona fide (human) and spoofed (AI-generated) speech.
+Uses original AASIST and fine-tuned AASIST models with heuristic pattern rules
+for improved multilingual detection (English + Indian languages).
 """
 
 import logging
-from typing import Dict
+from typing import Dict, List, Tuple
 
 import numpy as np
 import torch
 
 from config import settings
+from models.AASIST import Model as AASISTModel
 
 from .base import BaseDetector, PredictionResult
 
 logger = logging.getLogger(__name__)
 
+# AASIST model configuration (fixed for all weights)
+AASIST_MODEL_CONFIG = {
+    "architecture": "AASIST",
+    "nb_samp": 64600,
+    "first_conv": 128,
+    "filts": [70, [1, 32], [32, 32], [32, 64], [64, 64]],
+    "gat_dims": [64, 32],
+    "pool_ratios": [0.5, 0.7, 0.5, 0.5],
+    "temperatures": [2.0, 2.0, 100.0, 100.0],
+}
 
-class AASISTDetector(BaseDetector):
+
+class AASISTEnsembleDetector(BaseDetector):
     """
-    AASIST anti-spoofing detector for AI-generated voice detection.
+    Ensemble of original and fine-tuned AASIST models for deepfake detection.
 
-    Loads a TorchScript model at startup and runs inference on audio waveforms.
-    The model outputs logits for [spoof, bonafide] classes.
+    Uses pattern-based heuristics to handle inverted detection for Indian languages
+    where the original English-trained model gives reversed predictions.
     """
 
     def __init__(self):
-        self.model = None
+        self.original_model = None
+        self.finetuned_model = None
         self.device = None
         self._demo_mode = False
 
     @property
     def name(self) -> str:
-        return "aasist"
+        return "aasist_ensemble"
 
     def load(self) -> None:
         """
-        Load the AASIST TorchScript model from disk.
-
-        If the model file doesn't exist, enables demo mode with dummy responses.
+        Load both AASIST models from GCS or local cache.
 
-        Raises:
-            RuntimeError: If model file is .pth (requires architecture code)
+        Downloads weights from GCS if URIs are provided and files don't exist locally.
+        Falls back to demo mode if weights are not available.
         """
-        model_path = settings.AASIST_MODEL_PATH
-        device_str = settings.AASIST_DEVICE
-
-        logger.info(f"Loading AASIST model from: {model_path}")
-        logger.info(f"Using device: {device_str}")
+        from services.gcs_weights import download_weights
 
-        # Validate model file format
-        if model_path.endswith(".pth"):
-            raise RuntimeError(
-                f"Model file '{model_path}' is a .pth checkpoint. "
-                "Provide a TorchScript .pt file for deployment, "
-                "or vendor the AASIST architecture code in this repo."
-            )
+        device_str = settings.AASIST_DEVICE
+        logger.info(f"Initializing AASIST ensemble detector on device: {device_str}")
 
         # Set device
         if device_str == "cuda" and torch.cuda.is_available():
@@ -66,23 +68,47 @@ class AASISTDetector(BaseDetector):
                 logger.warning("CUDA requested but not available, falling back to CPU")
             self.device = torch.device("cpu")
 
-        # Load TorchScript model (or enable demo mode if not found)
+        # Download/load original model weights
+        orig_weights_path = settings.AASIST_ORIG_CACHE_PATH
+        orig_gcs_uri = settings.AASIST_ORIG_WEIGHTS_GCS_URI
+
+        # Download/load finetuned model weights
+        ft_weights_path = settings.AASIST_FT_CACHE_PATH
+        ft_gcs_uri = settings.AASIST_FT_WEIGHTS_GCS_URI
+
         try:
-            self.model = torch.jit.load(model_path, map_location=self.device)
-            self.model.eval()
+            # Download weights if needed
+            if orig_gcs_uri:
+                orig_weights_path = download_weights(orig_gcs_uri, orig_weights_path)
+            if ft_gcs_uri:
+                ft_weights_path = download_weights(ft_gcs_uri, ft_weights_path)
+
+            # Load original model
+            logger.info(f"Loading original AASIST model from: {orig_weights_path}")
+            self.original_model = self._load_model(orig_weights_path)
+
+            # Load finetuned model
+            logger.info(f"Loading fine-tuned AASIST model from: {ft_weights_path}")
+            self.finetuned_model = self._load_model(ft_weights_path)
+
             self._demo_mode = False
-            logger.info(f"AASIST model loaded successfully on {self.device}")
-        except (FileNotFoundError, ValueError) as e:
-            # Model file doesn't exist - enable demo mode
-            self._demo_mode = True
-            self.model = None
+            logger.info("AASIST ensemble models loaded successfully")
+
+        except Exception as e:
             logger.warning(
-                f"AASIST model not found at '{model_path}'. "
-                "Running in DEMO MODE with dummy responses. "
-                "Provide a valid model file for production use."
+                f"Failed to load AASIST models: {e}. Running in DEMO MODE."
             )
-        except Exception as e:
-            raise RuntimeError(f"Failed to load AASIST model: {e}")
+            self._demo_mode = True
+            self.original_model = None
+            self.finetuned_model = None
+
+    def _load_model(self, weights_path: str) -> torch.nn.Module:
+        """Load a single AASIST model from weights file."""
+        model = AASISTModel(AASIST_MODEL_CONFIG).to(self.device)
+        state_dict = torch.load(weights_path, map_location=self.device)
+        model.load_state_dict(state_dict)
+        model.eval()
+        return model
 
     def predict(
         self,
@@ -93,60 +119,181 @@ class AASISTDetector(BaseDetector):
         qc: Dict[str, float],
     ) -> PredictionResult:
         """
-        Run AASIST inference on the audio waveform.
+        Run ensemble inference on audio waveform.
+
+        Applies multi-window sampling for long audio and uses heuristic
+        pattern rules to handle inverted detection for Indian languages.
 
         Args:
-            language: Detected language (passed through, not used for inference).
-            mp3_bytes: Original MP3 bytes (not used, waveform is preferred).
-            waveform: Audio samples as float32 numpy array (mono, 16kHz).
-            sr: Sample rate (expected 16000 Hz).
-            qc: Quality control metrics dictionary.
+            language: Input language (Tamil, English, Hindi, Malayalam, Telugu)
+            mp3_bytes: Original MP3 bytes (not used)
+            waveform: Audio as float32 numpy array (mono, 16kHz)
+            sr: Sample rate (expected 16000)
+            qc: Quality control metrics
 
         Returns:
-            PredictionResult with classification, confidence, and explanation.
+            PredictionResult with classification, confidence, and explanation
         """
-        # Demo mode: return dummy responses
-        if self._demo_mode or self.model is None:
+        if self._demo_mode:
             return self._predict_demo(language, qc)
 
-        # Convert waveform to tensor: shape [1, T] (batch=1, time=T)
-        # Waveform is already float32 normalized to [-1, 1] from audio_io.py
-        audio_tensor = torch.from_numpy(waveform).unsqueeze(0).to(self.device)
+        # Get windows for multi-window inference
+        windows = self._get_audio_windows(waveform)
+
+        # Aggregate scores from all windows
+        all_orig_scores = []
+        all_ft_scores = []
+
+        for window in windows:
+            audio_tensor = torch.FloatTensor(window).unsqueeze(0).to(self.device)
+
+            with torch.no_grad():
+                # Original model scores
+                _, orig_output = self.original_model(audio_tensor)
+                orig_probs = torch.softmax(orig_output, dim=1)
+                orig_fake = orig_probs[0, 0].item()  # index 0 = spoof/fake
+                orig_real = orig_probs[0, 1].item()  # index 1 = real
+
+                # Finetuned model scores
+                _, ft_output = self.finetuned_model(audio_tensor)
+                ft_probs = torch.softmax(ft_output, dim=1)
+                ft_fake = ft_probs[0, 0].item()
+                ft_real = ft_probs[0, 1].item()
+
+            all_orig_scores.append((orig_real, orig_fake))
+            all_ft_scores.append((ft_real, ft_fake))
+
+        # Average scores across windows
+        avg_orig_real = np.mean([s[0] for s in all_orig_scores])
+        avg_orig_fake = np.mean([s[1] for s in all_orig_scores])
+        avg_ft_real = np.mean([s[0] for s in all_ft_scores])
+        avg_ft_fake = np.mean([s[1] for s in all_ft_scores])
+
+        # Apply ensemble heuristics
+        result = self._apply_ensemble_heuristics(
+            avg_orig_real, avg_orig_fake, avg_ft_real, avg_ft_fake, language
+        )
+
+        return result
+
+    def _get_audio_windows(self, waveform: np.ndarray) -> List[np.ndarray]:
+        """
+        Extract audio windows for multi-window inference.
+
+        For short audio, pads/tiles to target length.
+        For long audio, samples N evenly-spaced windows.
+
+        Args:
+            waveform: Input waveform as numpy array
+
+        Returns:
+            List of audio windows, each of length nb_samp
+        """
+        nb_samp = AASIST_MODEL_CONFIG["nb_samp"]  # 64600
+        max_windows = settings.AASIST_MAX_WINDOWS
+
+        if len(waveform) <= nb_samp:
+            # Pad or tile short audio
+            if len(waveform) >= nb_samp:
+                window = waveform[:nb_samp]
+            else:
+                num_repeats = int(np.ceil(nb_samp / len(waveform)))
+                window = np.tile(waveform, num_repeats)[:nb_samp]
+            return [window.astype(np.float32)]
 
-        # Run inference
-        with torch.no_grad():
-            # Model outputs logits: [batch, 2] where index 0=spoof, 1=bonafide
-            logits = self.model(audio_tensor)
+        # Long audio: sample evenly-spaced windows
+        total_len = len(waveform)
+        n_windows = min(max_windows, total_len // nb_samp)
+        n_windows = max(1, n_windows)
 
-            # Apply softmax to get probabilities
-            probs = torch.softmax(logits, dim=-1)
+        windows = []
+        step = (total_len - nb_samp) // max(1, n_windows - 1) if n_windows > 1 else 0
+
+        for i in range(n_windows):
+            start = i * step
+            end = start + nb_samp
+            if end <= total_len:
+                windows.append(waveform[start:end].astype(np.float32))
+
+        return windows if windows else [waveform[:nb_samp].astype(np.float32)]
+
+    def _apply_ensemble_heuristics(
+        self,
+        orig_real: float,
+        orig_fake: float,
+        ft_real: float,
+        ft_fake: float,
+        language: str,
+    ) -> PredictionResult:
+        """
+        Apply heuristic pattern rules for ensemble classification.
 
-            # Extract probabilities
-            # Index 0 = spoof (AI-generated), Index 1 = bonafide (human)
-            p_spoof = probs[0, 0].item()
-            p_bonafide = probs[0, 1].item()
+        Handles the inverted detection pattern for Indian languages where
+        the original English-trained model gives reversed predictions.
 
-        # Classification based on threshold
+        Pattern logic:
+        1. orig_fake > 95% + ft_real > 95% ΓåÆ REAL Indian (inverted)
+        2. orig_fake < 15% + ft_real > 80% ΓåÆ FAKE Indian (inverted)
+        3. orig_fake > 92% ΓåÆ FAKE English
+        4. orig_fake 85-92% + ft_real > 95% ΓåÆ REAL English
+        5. Default fallback
+        """
+        detected_pattern = ""
         threshold = settings.AASIST_THRESHOLD
 
-        if p_spoof > threshold:
-            classification = "AI_GENERATED"
-            confidence = p_spoof
-            explanation = (
-                f"AASIST anti-spoof model detected patterns consistent with "
-                f"synthetic speech (confidence: {confidence:.0%})."
-            )
+        # Pattern 1: Very high fake from original (>95%) + Fine-tuned confident real
+        # This indicates REAL Indian language audio (inverted detection)
+        if orig_fake > 0.95 and ft_real > 0.95:
+            is_fake = False
+            confidence = ft_real
+            detected_pattern = "Indian REAL (inverted detection)"
+
+        # Pattern 2: Very low fake from original (<15%) + Fine-tuned says real
+        # This indicates FAKE Indian language audio (inverted detection)
+        elif orig_fake < 0.15 and ft_real > 0.80:
+            is_fake = True
+            confidence = 1 - orig_fake  # Invert the score
+            detected_pattern = "Indian FAKE (inverted detection)"
+
+        # Pattern 3: Original confident fake (>92%) - likely English fake
+        elif orig_fake > 0.92:
+            is_fake = True
+            confidence = orig_fake
+            detected_pattern = "English FAKE (direct detection)"
+
+        # Pattern 4: Original moderate-high (85-92%) + Fine-tuned very confident real
+        # Likely English real audio
+        elif orig_fake > 0.85 and ft_real > 0.95:
+            is_fake = False
+            confidence = ft_real
+            detected_pattern = "English REAL (threshold)"
+
+        # Pattern 5: Default fallback
         else:
-            classification = "HUMAN"
-            confidence = p_bonafide
-            explanation = (
-                f"AASIST anti-spoof model indicates bona fide human speech "
-                f"(confidence: {confidence:.0%})."
-            )
+            # Use fine-tuned model's confidence if very high
+            if ft_real > 0.99 and orig_fake < 0.91:
+                is_fake = False
+                confidence = ft_real
+                detected_pattern = "Default REAL (fine-tuned confident)"
+            else:
+                # Weighted decision
+                is_fake = orig_fake > threshold
+                confidence = orig_fake if is_fake else orig_real
+                detected_pattern = "Default (weighted)"
 
-        # Ensure confidence is in valid range
+        # Build classification and explanation
+        classification = "AI_GENERATED" if is_fake else "HUMAN"
         confidence = max(0.0, min(1.0, confidence))
 
+        explanation = (
+            f"Ensemble: {detected_pattern}. "
+            f"Orig(fake={orig_fake:.0%}), FT(real={ft_real:.0%})"
+        )
+
+        # Truncate explanation to 200 chars
+        if len(explanation) > 200:
+            explanation = explanation[:197] + "..."
+
         return PredictionResult(
             classification=classification,
             confidenceScore=confidence,
@@ -154,37 +301,32 @@ class AASISTDetector(BaseDetector):
         )
 
     def _predict_demo(self, language: str, qc: Dict[str, float]) -> PredictionResult:
-        """
-        Return dummy prediction for demo mode.
-        
-        Uses a simple heuristic based on audio RMS to vary responses.
-        """
+        """Return dummy prediction for demo mode."""
         import random
-        
+
         # Use RMS to seed randomness for consistent results per audio
         rms = qc.get("rms", 0.1)
         random.seed(int(rms * 10000))
-        
+
         is_ai = random.random() > 0.5
-        
+
         if is_ai:
             confidence = round(random.uniform(0.70, 0.92), 2)
             explanation = (
                 f"[DEMO] Detected synthetic speech patterns in {language} audio. "
-                f"Model not loaded - using dummy response."
+                f"Models not loaded - using dummy response."
             )
             classification = "AI_GENERATED"
         else:
             confidence = round(random.uniform(0.68, 0.90), 2)
             explanation = (
-                f"[DEMO] Natural speech patterns detected in {language} audio. "
-                f"Model not loaded - using dummy response."
+                f"[DEMO] Natural speech patterns in {language} audio. "
+                f"Models not loaded - using dummy response."
             )
             classification = "HUMAN"
-        
+
         return PredictionResult(
             classification=classification,
             confidenceScore=confidence,
             explanation=explanation,
         )
-
diff --git a/detectors/registry.py b/detectors/registry.py
index 8b8e6f6..969a661 100644
--- a/detectors/registry.py
+++ b/detectors/registry.py
@@ -46,11 +46,11 @@ def initialize_detector() -> BaseDetector:
     """
     global _detector_instance
 
-    logger.info("Initializing AASIST detector")
+    logger.info("Initializing AASIST ensemble detector")
 
-    from .aasist_detector import AASISTDetector
+    from .aasist_detector import AASISTEnsembleDetector
 
-    detector = AASISTDetector()
+    detector = AASISTEnsembleDetector()
 
     # Load the model (will raise on failure)
     detector.load()
diff --git a/platform_services.py b/platform_services.py
index ef1d029..68ec19e 100644
--- a/platform_services.py
+++ b/platform_services.py
@@ -71,10 +71,12 @@ async def verify_api_key(api_key: Optional[str] = Depends(api_key_header)) -> st
             ).model_dump(),
         )
 
-    # Constant-time comparison for each valid key
+    # Constant-time comparison for each valid key (case-insensitive)
+    # Normalize both incoming key and stored keys to lowercase
     is_valid = False
+    api_key_lower = api_key.lower()
     for valid_key in valid_keys:
-        if hmac.compare_digest(api_key.encode("utf-8"), valid_key.encode("utf-8")):
+        if hmac.compare_digest(api_key_lower.encode("utf-8"), valid_key.lower().encode("utf-8")):
             is_valid = True
             break
 
diff --git a/requirements.txt b/requirements.txt
index 6a868ae..fa23002 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -4,6 +4,8 @@ pydantic>=2.5.0
 pydantic-settings>=2.1.0
 numpy>=1.26.0
 torch>=2.0.0
+google-cloud-storage>=2.10.0
+soundfile>=0.12.0
 pytest>=7.4.0
 httpx>=0.26.0
 python-dotenv>=1.0.0
\ No newline at end of file
diff --git a/scripts/deploy_cloudrun.sh b/scripts/deploy_cloudrun.sh
index bfafcad..f43888f 100644
--- a/scripts/deploy_cloudrun.sh
+++ b/scripts/deploy_cloudrun.sh
@@ -5,6 +5,7 @@
 # - gcloud CLI installed and authenticated
 # - Docker installed (for local builds)
 # - Project ID and region configured
+# - Model weights uploaded to GCS bucket
 
 set -e
 
@@ -20,9 +21,14 @@ IMAGE_NAME="gcr.io/${PROJECT_ID}/${SERVICE_NAME}"
 # API configuration
 VOICE_API_KEYS="${VOICE_API_KEYS:-your-production-api-key}"
 
-# AASIST model configuration
-AASIST_MODEL_PATH="${AASIST_MODEL_PATH:-/app/models/aasist_finetuned.pt}"
-AASIST_DEVICE="${AASIST_DEVICE:-cpu}"
+# AASIST ensemble model configuration
+AASIST_DEVICE="cpu"
+AASIST_ORIG_WEIGHTS_GCS_URI="${AASIST_ORIG_WEIGHTS_GCS_URI:-gs://your-bucket/aasist_original.pth}"
+AASIST_FT_WEIGHTS_GCS_URI="${AASIST_FT_WEIGHTS_GCS_URI:-gs://your-bucket/aasist_finetuned_best.pth}"
+AASIST_ORIG_CACHE_PATH="/tmp/aasist_original.pth"
+AASIST_FT_CACHE_PATH="/tmp/aasist_finetuned_best.pth"
+AASIST_MAX_WINDOWS="3"
+AASIST_THRESHOLD="0.5"
 
 # =============================================================================
 # BUILD AND DEPLOY
@@ -55,23 +61,28 @@ gcloud run deploy "${SERVICE_NAME}" \
     --platform managed \
     --allow-unauthenticated \
     --set-env-vars="VOICE_API_KEYS=${VOICE_API_KEYS}" \
-    --set-env-vars="AASIST_MODEL_PATH=${AASIST_MODEL_PATH}" \
     --set-env-vars="AASIST_DEVICE=${AASIST_DEVICE}" \
+    --set-env-vars="AASIST_ORIG_WEIGHTS_GCS_URI=${AASIST_ORIG_WEIGHTS_GCS_URI}" \
+    --set-env-vars="AASIST_FT_WEIGHTS_GCS_URI=${AASIST_FT_WEIGHTS_GCS_URI}" \
+    --set-env-vars="AASIST_ORIG_CACHE_PATH=${AASIST_ORIG_CACHE_PATH}" \
+    --set-env-vars="AASIST_FT_CACHE_PATH=${AASIST_FT_CACHE_PATH}" \
+    --set-env-vars="AASIST_MAX_WINDOWS=${AASIST_MAX_WINDOWS}" \
+    --set-env-vars="AASIST_THRESHOLD=${AASIST_THRESHOLD}" \
     --set-env-vars="ENABLE_DOCS=0" \
     --set-env-vars="MAX_MP3_BYTES=15000000" \
     --set-env-vars="MAX_DURATION_SECONDS=300" \
-    --memory=1Gi \
-    --cpu=1 \
-    --timeout=120s \
+    --memory=4Gi \
+    --cpu=2 \
+    --timeout=300s \
     --concurrency=1 \
     --min-instances=0 \
     --max-instances=10
 
 # Note on settings:
 # - concurrency=1: Each instance handles one request at a time (safer for ML models)
-# - timeout=120s: Allow up to 2 minutes per request for audio processing
-# - memory=1Gi: Increased for PyTorch model loading
-# - min-instances=0: Scale to zero when idle (cost savings)
+# - timeout=300s: Allow up to 5 minutes per request for audio processing
+# - memory=4Gi: Increased for PyTorch ensemble model loading (2 models)
+# - cpu=2: Two CPUs for ensemble inference
 
 echo ""
 echo "========================================"
@@ -79,4 +90,10 @@ echo "Deployment complete!"
 echo ""
 echo "Get the service URL with:"
 echo "  gcloud run services describe ${SERVICE_NAME} --region ${REGION} --format='value(status.url)'"
+echo ""
+echo "Test the API with:"
+echo "  curl -X POST <SERVICE_URL>/api/voice-detection \\"
+echo "    -H 'Content-Type: application/json' \\"
+echo "    -H 'x-api-key: ${VOICE_API_KEYS}' \\"
+echo "    -d '{\"language\": \"English\", \"audioFormat\": \"mp3\", \"audioBase64\": \"<BASE64_MP3>\"}'"
 echo "========================================"
diff --git a/scripts/test_curl.sh b/scripts/test_curl.sh
new file mode 100644
index 0000000..6c264f2
--- /dev/null
+++ b/scripts/test_curl.sh
@@ -0,0 +1,49 @@
+#!/bin/bash
+# Local test script using curl
+# 
+# Usage:
+#   ./test_curl.sh path/to/audio.mp3 [API_KEY] [URL]
+#
+# Example:
+#   ./test_curl.sh sample.mp3 test-key-123 http://localhost:8001
+
+set -e
+
+AUDIO_FILE="${1:-sample.mp3}"
+API_KEY="${2:-test-key-123}"
+BASE_URL="${3:-http://localhost:8001}"
+
+if [ ! -f "$AUDIO_FILE" ]; then
+    echo "Error: Audio file not found: $AUDIO_FILE"
+    echo "Usage: $0 <audio.mp3> [api-key] [base-url]"
+    exit 1
+fi
+
+echo "Testing Voice Detection API"
+echo "==========================="
+echo "Audio: $AUDIO_FILE"
+echo "URL: $BASE_URL"
+echo ""
+
+# Encode audio to base64
+AUDIO_BASE64=$(base64 -w 0 "$AUDIO_FILE" 2>/dev/null || base64 "$AUDIO_FILE")
+
+# Build JSON payload
+JSON_PAYLOAD=$(cat <<EOF
+{
+  "language": "English",
+  "audioFormat": "mp3",
+  "audioBase64": "$AUDIO_BASE64"
+}
+EOF
+)
+
+# Send request
+echo "Sending request..."
+curl -s -X POST "${BASE_URL}/api/voice-detection" \
+    -H "Content-Type: application/json" \
+    -H "x-api-key: ${API_KEY}" \
+    -d "$JSON_PAYLOAD" | python3 -m json.tool
+
+echo ""
+echo "Done."
diff --git a/scripts/test_local.py b/scripts/test_local.py
new file mode 100644
index 0000000..2baf773
--- /dev/null
+++ b/scripts/test_local.py
@@ -0,0 +1,108 @@
+#!/usr/bin/env python3
+"""
+Local test script for the Voice Detection API.
+
+Reads an MP3 file, encodes it to base64, and sends a request to the API.
+"""
+
+import argparse
+import base64
+import json
+import sys
+from pathlib import Path
+
+try:
+    import httpx
+except ImportError:
+    print("Error: httpx not installed. Run: pip install httpx")
+    sys.exit(1)
+
+
+def main():
+    parser = argparse.ArgumentParser(
+        description="Test the Voice Detection API with a local MP3 file"
+    )
+    parser.add_argument(
+        "--audio", 
+        type=str, 
+        required=True, 
+        help="Path to MP3 audio file"
+    )
+    parser.add_argument(
+        "--api-key", 
+        type=str, 
+        default="test-key-123",
+        help="API key for authentication (default: test-key-123)"
+    )
+    parser.add_argument(
+        "--url", 
+        type=str, 
+        default="http://localhost:8001",
+        help="API base URL (default: http://localhost:8001)"
+    )
+    parser.add_argument(
+        "--language", 
+        type=str, 
+        default="English",
+        choices=["Tamil", "English", "Hindi", "Malayalam", "Telugu"],
+        help="Language of the audio (default: English)"
+    )
+    args = parser.parse_args()
+
+    # Read and encode audio file
+    audio_path = Path(args.audio)
+    if not audio_path.exists():
+        print(f"Error: Audio file not found: {audio_path}")
+        sys.exit(1)
+
+    print(f"Reading audio file: {audio_path}")
+    with open(audio_path, "rb") as f:
+        audio_bytes = f.read()
+
+    audio_base64 = base64.b64encode(audio_bytes).decode("utf-8")
+    print(f"Audio size: {len(audio_bytes)} bytes ({len(audio_base64)} base64 chars)")
+
+    # Prepare request
+    endpoint = f"{args.url.rstrip('/')}/api/voice-detection"
+    headers = {
+        "Content-Type": "application/json",
+        "x-api-key": args.api_key,
+    }
+    payload = {
+        "language": args.language,
+        "audioFormat": "mp3",
+        "audioBase64": audio_base64,
+    }
+
+    # Send request
+    print(f"\nSending request to: {endpoint}")
+    print(f"Language: {args.language}")
+    print("-" * 50)
+
+    try:
+        with httpx.Client(timeout=120.0) as client:
+            response = client.post(endpoint, json=payload, headers=headers)
+
+        print(f"Status: {response.status_code}")
+        
+        result = response.json()
+        print(f"\nResponse:")
+        print(json.dumps(result, indent=2))
+
+        if response.status_code == 200:
+            print("-" * 50)
+            print(f"Classification: {result.get('classification', 'N/A')}")
+            print(f"Confidence: {result.get('confidenceScore', 'N/A')}")
+            print(f"Explanation: {result.get('explanation', 'N/A')}")
+
+    except httpx.ConnectError:
+        print(f"Error: Could not connect to {args.url}")
+        print("Make sure the server is running: uvicorn api:app --port 8001")
+        sys.exit(1)
+    except Exception as e:
+        print(f"Error: {e}")
+        sys.exit(1)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/services/__init__.py b/services/__init__.py
index e8467cc..a724955 100644
--- a/services/__init__.py
+++ b/services/__init__.py
@@ -1,10 +1,13 @@
-"""Services package for audio processing and quality control."""
+"""Audio I/O, QC, and GCS services for voice detection."""
 
 from .audio_io import AudioDecodeError, decode_mp3_to_waveform
+from .gcs_weights import download_weights, parse_gcs_uri
 from .qc import compute_qc_metrics
 
 __all__ = [
     "AudioDecodeError",
     "decode_mp3_to_waveform",
     "compute_qc_metrics",
+    "download_weights",
+    "parse_gcs_uri",
 ]
diff --git a/services/gcs_weights.py b/services/gcs_weights.py
new file mode 100644
index 0000000..3ccae75
--- /dev/null
+++ b/services/gcs_weights.py
@@ -0,0 +1,150 @@
+"""
+GCS weights download utility for AASIST ensemble models.
+
+Downloads model weights from Google Cloud Storage to local cache,
+with simple locking to prevent concurrent downloads in the same instance.
+"""
+
+import logging
+import os
+import time
+from pathlib import Path
+from typing import Tuple
+
+logger = logging.getLogger(__name__)
+
+
+def parse_gcs_uri(gcs_uri: str) -> Tuple[str, str]:
+    """
+    Parse a GCS URI into bucket and blob path.
+
+    Args:
+        gcs_uri: URI in format gs://bucket-name/path/to/file
+
+    Returns:
+        Tuple of (bucket_name, blob_path)
+
+    Raises:
+        ValueError: If URI format is invalid
+    """
+    if not gcs_uri.startswith("gs://"):
+        raise ValueError(f"Invalid GCS URI format: {gcs_uri}. Must start with 'gs://'")
+
+    # Remove gs:// prefix
+    path = gcs_uri[5:]
+
+    # Split bucket and blob path
+    parts = path.split("/", 1)
+    if len(parts) < 2 or not parts[1]:
+        raise ValueError(f"Invalid GCS URI format: {gcs_uri}. Must include bucket and blob path")
+
+    bucket_name = parts[0]
+    blob_path = parts[1]
+
+    return bucket_name, blob_path
+
+
+def download_weights(gcs_uri: str, local_path: str) -> str:
+    """
+    Download model weights from GCS to local cache.
+
+    Features:
+    - Skip download if local file exists and is non-empty
+    - Download to temp file first, then atomic rename
+    - Simple lockfile to prevent concurrent downloads
+
+    Args:
+        gcs_uri: GCS URI (gs://bucket/path) or empty string to skip
+        local_path: Local path to save weights
+
+    Returns:
+        Local path to the weights file
+
+    Raises:
+        RuntimeError: If download fails
+    """
+    if not gcs_uri:
+        logger.info(f"No GCS URI provided, expecting local file at: {local_path}")
+        return local_path
+
+    local_path = Path(local_path)
+    lock_path = Path(f"{local_path}.lock")
+    tmp_path = Path(f"{local_path}.tmp")
+
+    # Check if already downloaded
+    if local_path.exists() and local_path.stat().st_size > 0:
+        logger.info(f"Weights already cached at: {local_path}")
+        return str(local_path)
+
+    # Ensure parent directory exists
+    local_path.parent.mkdir(parents=True, exist_ok=True)
+
+    # Simple lockfile mechanism
+    max_wait = 300  # 5 minutes
+    wait_time = 0
+
+    while lock_path.exists() and wait_time < max_wait:
+        logger.info(f"Waiting for lock: {lock_path}")
+        time.sleep(5)
+        wait_time += 5
+
+        # Check if download completed while waiting
+        if local_path.exists() and local_path.stat().st_size > 0:
+            logger.info(f"Weights downloaded by another process: {local_path}")
+            return str(local_path)
+
+    # Acquire lock
+    try:
+        lock_path.touch()
+
+        # Check again after acquiring lock
+        if local_path.exists() and local_path.stat().st_size > 0:
+            logger.info(f"Weights already cached: {local_path}")
+            return str(local_path)
+
+        # Parse GCS URI
+        bucket_name, blob_path = parse_gcs_uri(gcs_uri)
+        logger.info(f"Downloading weights from gs://{bucket_name}/{blob_path}")
+
+        # Import GCS client (lazy import to avoid startup overhead if not needed)
+        try:
+            from google.cloud import storage
+        except ImportError:
+            raise RuntimeError(
+                "google-cloud-storage package not installed. "
+                "Run: pip install google-cloud-storage"
+            )
+
+        # Download from GCS
+        start_time = time.time()
+        client = storage.Client()
+        bucket = client.bucket(bucket_name)
+        blob = bucket.blob(blob_path)
+
+        # Download to temp file
+        blob.download_to_filename(str(tmp_path))
+
+        # Atomic rename
+        tmp_path.rename(local_path)
+
+        download_time = time.time() - start_time
+        file_size_mb = local_path.stat().st_size / (1024 * 1024)
+        logger.info(
+            f"Downloaded {file_size_mb:.1f}MB in {download_time:.1f}s: {local_path}"
+        )
+
+        return str(local_path)
+
+    except Exception as e:
+        # Clean up temp file on failure
+        if tmp_path.exists():
+            tmp_path.unlink()
+        raise RuntimeError(f"Failed to download weights from {gcs_uri}: {e}")
+
+    finally:
+        # Release lock
+        if lock_path.exists():
+            try:
+                lock_path.unlink()
+            except OSError:
+                pass
diff --git a/tests/conftest.py b/tests/conftest.py
index d1a923e..7e78213 100644
--- a/tests/conftest.py
+++ b/tests/conftest.py
@@ -15,31 +15,35 @@ sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
 # Set test environment variables BEFORE importing app
 os.environ["VOICE_API_KEYS"] = "test-key-123,test-key-456"
 os.environ["ENABLE_DOCS"] = "1"
-os.environ["AASIST_MODEL_PATH"] = "models/test_model.pt"  # Doesn't need to exist - we mock
 os.environ["AASIST_DEVICE"] = "cpu"
 os.environ["AASIST_THRESHOLD"] = "0.5"
+os.environ["AASIST_MAX_WINDOWS"] = "3"
+os.environ["AASIST_ORIG_WEIGHTS_GCS_URI"] = ""  # Empty = use local paths
+os.environ["AASIST_FT_WEIGHTS_GCS_URI"] = ""
+os.environ["AASIST_ORIG_CACHE_PATH"] = "/tmp/test_original.pth"
+os.environ["AASIST_FT_CACHE_PATH"] = "/tmp/test_finetuned.pth"
 
 # Import base classes before mocking
 from detectors.base import PredictionResult
 
 # Create mock detector instance
 _mock_detector = MagicMock()
-_mock_detector.name = "aasist"
+_mock_detector.name = "aasist_ensemble"
 _mock_detector.load = MagicMock()
 _mock_detector.predict = MagicMock(return_value=PredictionResult(
     classification="HUMAN",
     confidenceScore=0.75,
-    explanation="[TEST] Mocked AASIST detector result for testing."
+    explanation="[TEST] Mocked AASIST ensemble detector result for testing."
 ))
 
 # Import the aasist_detector module to ensure it's loaded
 import detectors.aasist_detector
 
-# Mock the AASISTDetector class
+# Mock the AASISTEnsembleDetector class
 _mock_aasist_class = MagicMock(return_value=_mock_detector)
 
 # Patch at module level so it's applied before app import
-_patcher = patch.object(detectors.aasist_detector, 'AASISTDetector', _mock_aasist_class)
+_patcher = patch.object(detectors.aasist_detector, 'AASISTEnsembleDetector', _mock_aasist_class)
 _patcher.start()
 
 # Now import the app (detector will be mocked)
